# NIFTY Intraday Price Prediction using Machine Learning

This project predicts the direction of the next candle's closing price using historical NIFTY intraday data and machine learning models.

## üéØ Objective

- Build ML models to predict if the next candle's close will be higher (1) or lower (0)
- Compare multiple models (Logistic Regression, Random Forest, XGBoost)
- Generate trading signals and calculate cumulative PnL
- Evaluate model performance using accuracy, precision, recall, and F1 score

## üìä Dataset

- **Source**: 1 year of NIFTY intraday OHLC data
- **Expected Columns**: `Timestamp`, `Open`, `High`, `Low`, `Close`, `Volume` (optional)
- **Location**: Place your CSV file in `data/raw/nifty_intraday.csv`

## üöÄ Setup Instructions

### 1. Clone or Download the Project

```bash
cd NIFTY_MODEL
```

### 2. Create Virtual Environment (Recommended)

```powershell
# Create virtual environment
python -m venv venv

# Activate virtual environment
.\venv\Scripts\Activate.ps1
```

### 3. Install Dependencies

```powershell
pip install -r requirements.txt
```

### 4. Add Your Data

Place your NIFTY intraday CSV file in the `data/raw/` folder with the name `nifty_intraday.csv`.

**Required columns:**
- `Timestamp` - Date and time of the candle
- `Open` - Opening price
- `High` - Highest price
- `Low` - Lowest price
- `Close` - Closing price
- `Volume` - Trading volume (optional)

## üìÅ Project Structure

```
NIFTY_MODEL/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Raw data files
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nifty_intraday.csv  # Your input data (add this)
‚îÇ   ‚îî‚îÄ‚îÄ processed/              # Processed data and predictions
‚îÇ       ‚îú‚îÄ‚îÄ train.csv
‚îÇ       ‚îú‚îÄ‚îÄ test.csv
‚îÇ       ‚îî‚îÄ‚îÄ final_predictions.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb           # Exploratory Data Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_model_experiments.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py         # Data loading and preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py # Technical indicators and features
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # ML model training
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py          # Model evaluation metrics
‚îÇ   ‚îî‚îÄ‚îÄ pnl_calculator.py      # Trading signals and PnL
‚îú‚îÄ‚îÄ models/                    # Saved trained models
‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression.pkl
‚îÇ   ‚îú‚îÄ‚îÄ random_forest.pkl
‚îÇ   ‚îú‚îÄ‚îÄ xgboost.pkl
‚îÇ   ‚îî‚îÄ‚îÄ scaler.pkl
‚îú‚îÄ‚îÄ results/                   # Plots and evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.csv
‚îÇ   ‚îú‚îÄ‚îÄ pnl_curve.png
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_*.png
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance_*.png
‚îÇ   ‚îî‚îÄ‚îÄ roc_curve_*.png
‚îú‚îÄ‚îÄ main.py                    # Main execution script
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## üèÉ Running the Project

### Option 1: Run the Complete Pipeline

Execute the main script to run the entire pipeline:

```powershell
python main.py
```

This will:
1. ‚úÖ Load and preprocess data
2. ‚úÖ Create target variable (1 if next close > current close, else 0)
3. ‚úÖ Engineer 70+ technical features
4. ‚úÖ Split data chronologically (70% train, 30% test)
5. ‚úÖ Train 3 ML models (Logistic Regression, Random Forest, XGBoost)
6. ‚úÖ Evaluate and compare models
7. ‚úÖ Select best model
8. ‚úÖ Generate trading signals
9. ‚úÖ Calculate cumulative PnL
10. ‚úÖ Save results and visualizations

### Option 2: Use Jupyter Notebooks

For interactive exploration:

```powershell
jupyter notebook
```

Then open:
- `01_eda.ipynb` - Exploratory Data Analysis
- `02_feature_engineering.ipynb` - Feature Engineering
- `03_model_experiments.ipynb` - Model Training and Experiments

## üìä Features Generated

### Technical Indicators (20+)
- **Momentum**: RSI, MACD, Stochastic Oscillator
- **Trend**: SMA (5, 10, 20), EMA (5, 10, 20)
- **Volatility**: Bollinger Bands, ATR
- **Volume**: Volume change percentage
- **Others**: ADX (trend strength)

### Candlestick Features (8)
- Body size, upper/lower wicks, candle range
- Bullish/bearish indicator
- Body-to-range ratio, wick ratios

### Lag Features (16)
- Previous 1, 2, 3, 5 candle closes
- Previous returns, highs, lows

### Rolling Features (15)
- Rolling mean, std, max, min (windows: 5, 10, 20)
- Rolling range

### Temporal Features (4)
- Hour, minute, day of week
- Market hours indicator

### Price Position Features (5)
- Distance from moving averages
- Bollinger Band position
- Distance from recent highs/lows

**Total: 70+ Features**

## ü§ñ Models Used

1. **Logistic Regression** - Baseline linear model
2. **Random Forest** - Ensemble tree-based model
3. **XGBoost** - Gradient boosting (typically best performer)

## üìà Evaluation Metrics

- **Accuracy** - Overall correctness
- **Precision** - Accuracy of positive predictions
- **Recall** - Coverage of actual positives
- **F1 Score** - Harmonic mean of precision and recall
- **Confusion Matrix** - Visual breakdown of predictions
- **ROC Curve & AUC** - Model discrimination ability
- **Feature Importance** - Most influential features

## üí∞ PnL Calculation Strategy

**Trading Logic:**
- **Buy Signal (Prediction = 1)**: Take long position at current close, exit at next close
  - PnL = Next Close - Current Close
- **Sell Signal (Prediction = 0)**: Take short position at current close, exit at next close
  - PnL = Current Close - Next Close

**PnL Metrics:**
- Cumulative PnL over test period
- Win rate (% of profitable trades)
- Average win/loss
- Maximum drawdown

## üì§ Output Files

After running `main.py`, you'll get:

### 1. Final Predictions CSV
**File**: `data/processed/final_predictions.csv`

Columns:
- `Timestamp` - Time of prediction
- `Close` - Actual closing price
- `Predicted` - Model prediction (0 or 1)
- `model_call` - Trading signal ('buy' or 'sell')
- `model_pnl` - Cumulative PnL

### 2. Model Comparison
**File**: `results/model_comparison.csv`
- Comparison of all models with metrics

### 3. Visualizations
- `pnl_curve.png` - Cumulative PnL over time
- `trade_distribution.png` - Distribution of individual trades
- `confusion_matrix_*.png` - Confusion matrices for each model
- `feature_importance_*.png` - Top features for tree models
- `roc_curve_*.png` - ROC curves for all models

### 4. Saved Models
All trained models saved in `models/` folder:
- `logistic_regression.pkl`
- `random_forest.pkl`
- `xgboost.pkl`
- `scaler.pkl`

## üîß Customization

### Change Train-Test Split Ratio

In `main.py`, modify:
```python
X_train, X_test, y_train, y_test, test_df = train_test_split_timeseries(df, train_ratio=0.7)
```

### Add More Features

Edit `src/feature_engineering.py` to add custom features.

### Tune Model Parameters

Edit `src/models.py` to adjust hyperparameters:
```python
model = XGBClassifier(
    n_estimators=200,      # More trees
    max_depth=8,           # Deeper trees
    learning_rate=0.05,    # Slower learning
    # ... other parameters
)
```

### Change Data Path

In `main.py`:
```python
data_path = 'data/raw/your_custom_file.csv'
```

## üìù Example Usage

```python
# Load a saved model and make predictions
from src.models import load_model
from sklearn.preprocessing import StandardScaler
import joblib

# Load model and scaler
model = load_model('xgboost')
scaler = joblib.load('models/scaler.pkl')

# Prepare your new data (with same features)
# X_new = ... your feature-engineered data

# Scale and predict
X_new_scaled = scaler.transform(X_new)
predictions = model.predict(X_new_scaled)
```

## ‚ö†Ô∏è Important Notes

1. **Time-Series Split**: The data is split chronologically (no shuffling) to prevent look-ahead bias
2. **Feature Engineering**: NaN rows are dropped after indicator calculation (~20-50 rows)
3. **Scaling**: Always scale test data using the scaler fitted on training data only
4. **Target Variable**: Last row is dropped (no next candle to predict)

## üêõ Troubleshooting

### Missing Data File
```
‚ùå ERROR: Data file not found at data/raw/nifty_intraday.csv
```
**Solution**: Place your CSV file in `data/raw/` folder with correct name.

### Missing Columns
```
‚ùå ERROR: Required columns not found
```
**Solution**: Ensure CSV has: Timestamp, Open, High, Low, Close

### Import Errors
```
‚ùå ERROR: No module named 'pandas_ta'
```
**Solution**: Install dependencies: `pip install -r requirements.txt`

### Memory Issues
If dataset is very large, reduce features or use sampling:
```python
df = df.sample(frac=0.5, random_state=42)  # Use 50% of data
```

## üìö Dependencies

- pandas 2.0.3
- numpy 1.24.3
- scikit-learn 1.3.0
- xgboost 2.0.0
- matplotlib 3.7.1
- seaborn 0.12.2
- pandas-ta 0.3.14b
- joblib 1.3.1
- ta 0.11.0

## üìä Performance Tips

1. **More data is better** - At least 6 months recommended
2. **Feature selection** - Remove low-importance features for faster training
3. **Hyperparameter tuning** - Use GridSearchCV for optimal parameters
4. **Ensemble methods** - Combine predictions from multiple models
5. **Cross-validation** - Use time-series cross-validation for robust evaluation

## ü§ù Contributing

Feel free to:
- Add more technical indicators
- Implement additional models (Neural Networks, SVM, etc.)
- Enhance PnL calculation with transaction costs
- Add risk management features (stop-loss, position sizing)

## üìÑ License

This project is for educational purposes. Use at your own risk. Not financial advice.

## üéì Learning Resources

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [pandas-ta Documentation](https://github.com/twopirllc/pandas-ta)
- [Technical Analysis Library](https://technical-analysis-library-in-python.readthedocs.io/)

## üìß Contact

For questions or issues, please open an issue in the repository.

---

**Happy Trading! üöÄüìà**

*Remember: Past performance does not guarantee future results. Always practice proper risk management.*
