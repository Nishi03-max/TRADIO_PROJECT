{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from src.data_loader import load_data, create_target_column, train_test_split_timeseries\n",
    "from src.feature_engineering import prepare_features\n",
    "from src.models import train_all_models, save_model\n",
    "from src.evaluation import evaluate_model, compare_models, plot_confusion_matrix, plot_feature_importance\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744b01c",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = load_data('../data/raw/nifty_intraday.csv')\n",
    "df = create_target_column(df)\n",
    "print(f\"Data loaded: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a4634",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "df = prepare_features(df)\n",
    "print(f\"Features created: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5fb1b",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data chronologically\n",
    "X_train, X_test, y_train, y_test, test_df = train_test_split_timeseries(df, train_ratio=0.7)\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625a08b",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"Train mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"Train std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c705c6",
   "metadata": {},
   "source": [
    "## 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971980df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "models = train_all_models(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebb12e",
   "metadata": {},
   "source": [
    "## 6. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ad18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Sell (0)', 'Buy (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0679086",
   "metadata": {},
   "source": [
    "## 7. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ba6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "best_model_name = compare_models(models, X_test_scaled, y_test)\n",
    "best_model = models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f516556",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40985ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Sell (0)', 'Buy (1)'],\n",
    "                yticklabels=['Sell (0)', 'Buy (1)'])\n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {result[\"accuracy\"]:.4f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50cfad3",
   "metadata": {},
   "source": [
    "## 9. Feature Importance (Random Forest & XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost']\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in models:\n",
    "        model = models[model_name]\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = model.feature_importances_\n",
    "        feature_imp_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot top 20\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_20 = feature_imp_df.head(20)\n",
    "        plt.barh(range(len(top_20)), top_20['importance'])\n",
    "        plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.title(f'Top 20 Feature Importances - {model_name}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTop 10 features for {model_name}:\")\n",
    "        print(feature_imp_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba631d2",
   "metadata": {},
   "source": [
    "## 10. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions from best model\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "print(pd.Series(best_predictions).value_counts())\n",
    "\n",
    "# Compare with actual distribution\n",
    "print(f\"\\nActual Distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fa402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs actual distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual\n",
    "actual_counts = y_test.value_counts().sort_index()\n",
    "axes[0].bar(['Sell (0)', 'Buy (1)'], actual_counts.values, color=['red', 'green'])\n",
    "axes[0].set_title('Actual Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predicted\n",
    "pred_counts = pd.Series(best_predictions).value_counts().sort_index()\n",
    "axes[1].bar(['Sell (0)', 'Buy (1)'], pred_counts.values, color=['red', 'green'])\n",
    "axes[1].set_title(f'Predicted Distribution ({best_model_name})', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb746a62",
   "metadata": {},
   "source": [
    "## 11. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Buy Predictions': sum(result['predictions'] == 1),\n",
    "        'Sell Predictions': sum(result['predictions'] == 0)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935313f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Model'], comparison_df['Accuracy'], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (model, acc) in enumerate(zip(comparison_df['Model'], comparison_df['Accuracy'])):\n",
    "    plt.text(i, acc + 0.02, f'{acc:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8509e833",
   "metadata": {},
   "source": [
    "## 12. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7fa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import joblib\n",
    "\n",
    "print(f\"Saving best model: {best_model_name}\")\n",
    "save_model(best_model, 'best_model')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "\n",
    "print(\"\\nModel and scaler saved successfully!\")\n",
    "print(f\"  - Best Model: ../models/best_model.pkl\")\n",
    "print(f\"  - Scaler: ../models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd2539",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "### Models Trained:\n",
    "1. **Logistic Regression** - Baseline linear model\n",
    "2. **Random Forest** - Ensemble of decision trees\n",
    "3. **XGBoost** - Gradient boosting model\n",
    "\n",
    "### Key Findings:\n",
    "- Best model identified based on accuracy\n",
    "- Feature importance analyzed for tree-based models\n",
    "- Confusion matrices show prediction patterns\n",
    "- Models saved for future use\n",
    "\n",
    "### Next Steps:\n",
    "1. Generate trading signals using best model\n",
    "2. Calculate PnL based on predictions\n",
    "3. Analyze trading performance\n",
    "4. Optimize strategy parameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
